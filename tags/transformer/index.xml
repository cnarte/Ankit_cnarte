<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer on Ankit Kumar</title>
    <link>https://cnarte.github.io/Ankit_cnarte/tags/transformer/</link>
    <description>Recent content in Transformer on Ankit Kumar</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://cnarte.github.io/Ankit_cnarte/tags/transformer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Bert Implimentaion in real machinehack dataset</title>
      <link>https://cnarte.github.io/Ankit_cnarte/post/project-4/</link>
      <pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://cnarte.github.io/Ankit_cnarte/post/project-4/</guid>
      <description>This is an explanation on how to implement bert on different devices like CPU and GPU. To check and install for cuda compatible devices, I think its documentation is far more appropriate. Many other places where Bert models can be deployed majorly depends on the type of data that we choose. Like if we want to deploy it in an app or just as a part of biger model architecture for recomendation system or speech generation models.</description>
    </item>
    
  </channel>
</rss>
